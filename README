### 1. GPU Fundamentals and Architecture

This section covers the "why" and "how" of GPUs, focusing on the hardware differences between CPUs and GPUs and the underlying execution model.

- **[Lecture 1 - Introduction to GPUs](https://canvas.northwestern.edu/courses/246748/files/23649853?wrap=1)**
- **[Lecture 3 - Architecture Overview and Threading Primer](https://canvas.northwestern.edu/courses/246748/files/23623474?wrap=1)**

### 2. CUDA Programming Basics

These lectures introduce the syntax and logic required to write programs that run on NVIDIA hardware, focusing on the hierarchy of threads, blocks, and grids.

- **[Lecture 2 - Introduction to CUDA Programming I](https://canvas.northwestern.edu/courses/246748/files/23623374?wrap=1)**
- **[Lecture 3 - Introduction to CUDA Programming II](https://canvas.northwestern.edu/courses/246748/files/23623474?wrap=1)**
- **[Lecture 4b - Introduction to CUDA Programming III](https://canvas.northwestern.edu/courses/246748/files/23623488?wrap=1)**

### 3. Performance Optimization Strategies

This is a significant core of the course, focusing on how to make CUDA code run efficiently by managing hardware constraints like memory bandwidth and execution flow.

- **[Lecture 4a/5/6 - Optimizing for CUDA](https://canvas.northwestern.edu/courses/246748/files/23623475?wrap=1)** (Control Divergence, Memory Coalescing, Tiling)
- **[Lecture 11 - High Performance at Low Occupancy](https://canvas.northwestern.edu/courses/246748/files/23623373?wrap=1)**
- **[Lecture 13 - Advanced Blocking / Tiling](https://canvas.northwestern.edu/courses/246748/files/23623358?wrap=1)**

### 4. Profiling and Numerical Considerations

These topics provide the tools to measure performance and the theoretical knowledge to handle floating-point math correctly in a parallel environment.

- **[Lecture 7 - GPU Performance Profiling](https://canvas.northwestern.edu/courses/246748/files/23623473?wrap=1)** (NVIDIA Nsight Systems)
- **[Lecture 19 - Floating-Point Considerations](https://canvas.northwestern.edu/courses/246748/files/23623439?wrap=1)**
- **[Lecture 16 - Memory Consistency](https://canvas.northwestern.edu/courses/246748/files/23623448?wrap=1)**

### 5. Parallel Patterns and Primitive Algorithms

This section explores classic parallel algorithms that serve as building blocks for more complex scientific and graphics applications.

- **[Lecture 8/9 - Parallel Reduction](https://canvas.northwestern.edu/courses/246748/files/23623478?wrap=1)**
- **[Lecture 10 - Histograms](https://canvas.northwestern.edu/courses/246748/files/23623480?wrap=1)**
- **[Lecture 12 - Parallel Prefix Scan](https://canvas.northwestern.edu/courses/246748/files/23623356?wrap=1)**
- **[Lecture 18 - Input Binning](https://canvas.northwestern.edu/courses/246748/files/23623450?wrap=1)**

### 6. Specialized Data Structures and Alternative Frameworks

The final grouping covers handling non-standard data types (like sparse matrices) and introduces other ways to program GPUs beyond CUDA.

- **[Lecture 14 - Vector Programming](https://canvas.northwestern.edu/courses/246748/files/23623359?wrap=1)**
- **[Lecture 15 - Textures](https://canvas.northwestern.edu/courses/246748/files/23623355?wrap=1)**
- **[Lecture 16 - Sparse Array Multiplication](https://canvas.northwestern.edu/courses/246748/files/23623448?wrap=1)**
- **[Lecture 20 - Introduction to OpenCL](https://canvas.northwestern.edu/courses/246748/files/23623449?wrap=1)**